{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T11:25:11.171489Z",
     "start_time": "2020-09-28T11:25:11.165479Z"
    }
   },
   "source": [
    "## Face Detection with Count and Distance Measurement\n",
    "### Importing modules"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Created by :Nithin C\n",
    "dated:05-02-2020\n",
    "Title:Social Distancing evaluation in real time and CCTV(recorded) Fotage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting which camera to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T08:49:35.519228Z",
     "start_time": "2021-02-08T08:49:33.725864Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#numerical calculations\n",
    "from scipy.spatial import distance as dist\n",
    "#scientific calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T08:49:38.338478Z",
     "start_time": "2021-02-08T08:49:35.977617Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2 #computer vision library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T08:49:40.292838Z",
     "start_time": "2021-02-08T08:49:40.277929Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T08:49:41.269453Z",
     "start_time": "2021-02-08T08:49:40.995416Z"
    }
   },
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)  #0 is webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T07:40:43.665960Z",
     "start_time": "2021-01-23T07:40:08.293309Z"
    }
   },
   "source": [
    "cap = cv2.VideoCapture(0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T07:40:53.839920Z",
     "start_time": "2021-01-23T07:40:51.016766Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "face_model = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Capturing the photo and showing image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T07:42:31.117967Z",
     "start_time": "2021-01-23T07:40:59.435148Z"
    }
   },
   "outputs": [],
   "source": [
    "status , photo = cap.read() # two values 1st value will be status:whether the camera is on are not,\n",
    "#2nd statement photo whgich is captured\n",
    "cv2.imshow('Socail Distancing' , photo)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture video and Show\n",
    "### Count and measurment logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-04dcd4529ffb>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-04dcd4529ffb>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    sudo apt install libgtk2.0-dev pkg-config\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "sudo apt install libgtk2.0-dev pkg-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T07:47:04.741336Z",
     "start_time": "2021-01-23T07:43:18.493632Z"
    }
   },
   "outputs": [],
   "source": [
    "while True: \n",
    "    status , photo = cap.read() \n",
    "    face_cor = face_model.detectMultiScale(photo)\n",
    "    l = len(face_cor)\n",
    "    photo = cv2.putText(photo, str(len(face_cor))+\" Face\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX,  \n",
    "                   1, (255, 0, 0) , 2, cv2.LINE_AA) \n",
    "    stack_x = [] \n",
    "    stack_y = []\n",
    "    stack_x_print = [] \n",
    "    stack_y_print = []\n",
    "    global D\n",
    "    \n",
    "    if len(face_cor) == 0:\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        for i in range(0,len(face_cor)):\n",
    "            x1 = face_cor[i][0]\n",
    "            y1 = face_cor[i][1]\n",
    "            x2 = face_cor[i][0] + face_cor[i][2]\n",
    "            y2 = face_cor[i][1] + face_cor[i][3]\n",
    "            \n",
    "             \n",
    "            \n",
    "            mid_x = int((x1+x2)/2)\n",
    "            mid_y = int((y1+y2)/2)\n",
    "            stack_x.append(mid_x)\n",
    "            stack_y.append(mid_y)\n",
    "            stack_x_print.append(mid_x)\n",
    "            stack_y_print.append(mid_y)\n",
    "            \n",
    "            photo = cv2.circle(photo, (mid_x, mid_y), 3 , [255,0,0] , -1) \n",
    "            photo = cv2.rectangle(photo , (x1, y1) , (x2,y2) , [0,255,0] , 2)\n",
    "        \n",
    "        if len(face_cor) == 2:\n",
    "            D = int(dist.euclidean((stack_x.pop(), stack_y.pop()), (stack_x.pop(), stack_y.pop())))\n",
    "            photo = cv2.line(photo, (stack_x_print.pop(), stack_y_print.pop()), (stack_x_print.pop(), stack_y_print.pop()), [0,0,255], 2)\n",
    "        else:\n",
    "            D = 0\n",
    "        \n",
    "        if D<250 and D!=0:\n",
    "            photo = cv2.putText(photo, \"!!MOVE AWAY!!\", (120, 100), cv2.FONT_HERSHEY_SIMPLEX,2, [0,0,255] , 4)\n",
    "            \n",
    "        photo = cv2.putText(photo, str(D/10) + \" cm\", (300, 50), cv2.FONT_HERSHEY_SIMPLEX,  \n",
    "                   1, (255, 0, 0) , 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow('hi' , photo)\n",
    "        if cv2.waitKey(100) == 13:\n",
    "            break\n",
    "            \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Releasing the Camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-23T07:47:05.276117Z",
     "start_time": "2021-01-23T07:47:04.839483Z"
    }
   },
   "outputs": [],
   "source": [
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second part of the Project for CCTV analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T08:49:48.635849Z",
     "start_time": "2021-02-08T08:49:48.419971Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as dist\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T08:49:56.623200Z",
     "start_time": "2021-02-08T08:49:56.619084Z"
    }
   },
   "outputs": [],
   "source": [
    "# base path to YOLO directory\n",
    "MODEL_PATH = \"/social_distance_analyzer-master/social_distance_analyzer-master/yolo-coco\"\n",
    "\n",
    "# initialize minimum probability to filter weak detections along with\n",
    "# the threshold when applying non-maxima suppression\n",
    "MIN_CONF = 0.3\n",
    "NMS_THRESH = 0.3\n",
    "\n",
    "# boolean indicating if NVIDIA CUDA GPU should be used\n",
    "USE_GPU = False\n",
    "\n",
    "# define the minimum safe distance (in pixels) that two people can be\n",
    "# from each other\n",
    "MIN_DISTANCE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T08:50:06.430323Z",
     "start_time": "2021-02-08T08:50:03.979853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
      "80\n",
      "[INFO] loading YOLO from disk...\n"
     ]
    }
   ],
   "source": [
    "# load the COCO class labels our YOLO model was trained on\n",
    "labelsPath = 'social_distance_analyzer-master/social_distance_analyzer-master/yolo-coco/coco.names'\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "\n",
    "print(LABELS)\n",
    "\n",
    "print(len(LABELS))\n",
    "\n",
    "# derive the paths to the YOLO weights and model configuration\n",
    "weightsPath = 'social_distance_analyzer-master/social_distance_analyzer-master/yolo-coco/yolov3.weights'\n",
    "configPath = 'social_distance_analyzer-master/social_distance_analyzer-master/yolo-coco/yolov3.cfg'\n",
    "\n",
    "# load our YOLO object detector trained on COCO dataset (80 classes)\n",
    "print(\"[INFO] loading YOLO from disk...\")\n",
    "net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T08:50:08.184350Z",
     "start_time": "2021-02-08T08:50:08.181040Z"
    }
   },
   "outputs": [],
   "source": [
    "if USE_GPU:\n",
    "    # set CUDA as the preferable backend and target\n",
    "    print(\"Setting preferable backend and target to CUDA...\")\n",
    "    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T08:50:11.594976Z",
     "start_time": "2021-02-08T08:50:11.574043Z"
    }
   },
   "outputs": [],
   "source": [
    "# determine only the *output* layer names that we need from YOLO\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T08:50:15.955066Z",
     "start_time": "2021-02-08T08:50:15.945341Z"
    }
   },
   "source": [
    "## Section 3: Input access (Video file and Live footage)\n",
    "The following section concentrates on building a block that accepts already existing video file or a live footage as an input for the algorithm.\n",
    "\n",
    "[Note : Use the below block or the block after that dont use both]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-08T08:51:39.700582Z",
     "start_time": "2021-02-08T08:51:39.500678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS of the current video:  0.0\n",
      "Number of frames in the video:  0.0\n"
     ]
    }
   ],
   "source": [
    "#upload the video file you want to check social distancing for- below.\n",
    "vs = cv2.VideoCapture('social_distance_analyzer-master/social_distance_analyzer-master/5_6127594218744447539.mp4')\n",
    "#src.upload(vs)\n",
    "fps = vs.get(cv2.CAP_PROP_FPS)\n",
    "print(\"FPS of the current video: \",fps)\n",
    "\n",
    "num_frames = vs.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "print(\"Number of frames in the video: \",num_frames)\n",
    "\n",
    "writer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Algorithm Detecting people¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_people(frame, net, ln, personIdx=0):\n",
    "\n",
    "    # grab the dimensions of the frame and  initialize the list of\n",
    "    # results\n",
    "    (H, W) = frame.shape[:2]\n",
    "    results = []\n",
    "\n",
    "    # construct a blob from the input frame and then perform a forward\n",
    "    # pass of the YOLO object detector, giving us our bounding boxes\n",
    "    # and associated probabilities\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    layerOutputs = net.forward(ln)\n",
    "\n",
    "    # initialize our lists of detected bounding boxes, centroids, and\n",
    "    # confidences, respectively\n",
    "    boxes = []\n",
    "    centroids = []\n",
    "    confidences = []\n",
    "\n",
    "    # loop over each of the layer outputs\n",
    "    for output in layerOutputs:\n",
    "        # loop over each of the detections\n",
    "        for detection in output:\n",
    "            # extract the class ID and confidence (i.e., probability)\n",
    "            # of the current object detection\n",
    "            scores = detection[5:]\n",
    "            classID = np.argmax(scores)\n",
    "            confidence = scores[classID]\n",
    "\n",
    "            # filter detections by (1) ensuring that the object\n",
    "            # detected was a person and (2) that the minimum\n",
    "            # confidence is met\n",
    "            if classID == personIdx and confidence > MIN_CONF:\n",
    "                # scale the bounding box coordinates back relative to\n",
    "                # the size of the image, keeping in mind that YOLO\n",
    "                # actually returns the center (x, y)-coordinates of\n",
    "                # the bounding box followed by the boxes' width and\n",
    "                # height\n",
    "                box = detection[0:4] * np.array([W, H, W, H])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "                # use the center (x, y)-coordinates to derive the top\n",
    "                # and and left corner of the bounding box\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "\n",
    "                # update our list of bounding box coordinates,\n",
    "                # centroids, and confidences\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                centroids.append((centerX, centerY))\n",
    "                confidences.append(float(confidence))\n",
    "\n",
    "    # apply non-maxima suppression to suppress weak, overlapping\n",
    "    # bounding boxes\n",
    "    idxs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)\n",
    "\n",
    "    # ensure at least one detection exists\n",
    "    if len(idxs) > 0:\n",
    "        # loop over the indexes we are keeping\n",
    "        for i in idxs.flatten():\n",
    "            # extract the bounding box coordinates\n",
    "            (x, y) = (boxes[i][0], boxes[i][1])\n",
    "            (w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "            # update our results list to consist of the person\n",
    "            # prediction probability, bounding box coordinates,\n",
    "            # and the centroid\n",
    "            r = (confidences[i], (x, y, x + w, y + h), centroids[i])\n",
    "            results.append(r)\n",
    "\n",
    "    # return the list of results\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
